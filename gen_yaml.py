#!/usr/bin/env python3

import base64
import logging
import multiprocessing
import random
import socket
import threading
import time
import urllib.parse

import emoji
import geoip2.database
import requests
import yaml
from requests.adapters import HTTPAdapter

import decode_url

# è½½å…¥ MaxMind æä¾›çš„æ•°æ®åº“æ–‡ä»¶
reader = geoip2.database.Reader('GeoLite2-Country.mmdb')

# é…ç½®æ—¥å¿—è®°å½•å™¨
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(lineno)d - %(message)s')

url_file = "./sub/url.txt"
server_host = 'http://127.0.0.1:25500'
# server_host = 'http://192.168.100.1:25500'
# config_url = 'https://raw.githubusercontent.com/zzcabc/Rules/master/MyConvert/MyRules.ini'

include = ".*é¦™æ¸¯.*|.*HK.*|.*Hong Kong.*|.*ğŸ‡­ğŸ‡°.*"
exclude = ".*æµ‹é€Ÿ.*|.*ç¦æ­¢.*|.*è¿‡æœŸ.*|.*å‰©ä½™.*|.*CN.*|.*å¤‡ç”¨.*|.*ğŸ‡¨ğŸ‡³.*"

exce_url = ['1.1.1.1', '8.8.8.8', '0.0.0.0',
            '127.0.0.1', '127.0.0.2', 'google.com', 'localhost', 'github.com']

with open(url_file, 'r', encoding='utf-8') as f:  # è½½å…¥è®¢é˜…é“¾æ¥
    urls = f.read()
    f.close()

url_list = urls.split("|")
# æ‰“ä¹±é¡ºåº
random.shuffle(url_list)
step = 30
index = 0
length = len(url_list)

thread_num = length // step + 1

# æ ¹æ®merged_proxies.yamlè¡¥å……æ‰€æœ‰åŠ å¯†ç®—æ³•
cipher_list = [
    'aes-128-gcm', 'aes-256-gcm', 'chacha20-ietf-poly1305', 'auto',
    'aes-256-cfb', 'aes-256-ctr', 'rc4-md5', 'xchacha20-ietf-poly1305', 'dummy'
]  # Define valid ciphers

# lock = threading.Lock()


def has_emoji(text):
    return emoji.emoji_count(text) != 0


def get_country_emoji(ip_address):
    try:
        ip_address = socket.gethostbyname(ip_address)
        # æŸ¥è¯¢ IP åœ°å€çš„åœ°ç†ä½ç½®ä¿¡æ¯
        response = reader.country(ip_address)
        # è·å–å›½å®¶ä»£ç 
        country_code = response.country.iso_code
        # å°†å›½å®¶ä»£ç è½¬æ¢ä¸º emoji
        if country_code:
            # å›½å®¶ä»£ç è½¬æ¢ä¸º emoji
            emoji_char = chr(ord(country_code[0]) + 127397) + chr(ord(country_code[1]) + 127397)
            logging.debug(f"{ip_address} emoji is {emoji_char}")
            return emoji_char
        else:
            logging.debug(f"{ip_address} emoji is None")
            return "ğŸŒ"
    except Exception as e:
        logging.error(f"{e}")
        return "ğŸŒ"


def test_connection(ip, port):
    # åˆ›å»º socket å¯¹è±¡
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º 2 ç§’
        sock.settimeout(2)
        # å°è¯•è¿æ¥åˆ°æŒ‡å®šçš„ IP åœ°å€å’Œç«¯å£
        result = sock.connect_ex((socket.gethostbyname(ip), port))
        if result == 0:
            return True
        else:
            return False
    except socket.error as e:
        logging.error(f"{ip}, {e}")
    finally:
        sock.close()


def run(index, shared_list):
    # print(threading.current_thread().getName(), "å¼€å§‹å·¥ä½œ")
    # for i in range(0, length, step):
    yaml_file = "./sub/" + str(index) + ".yaml"
    cur = index * step
    i = (index + 1) * step
    if i >= length:
        url_lists = url_list[cur:length]
    else:
        url_lists = url_list[cur:i]
    not_proxies = set()
    new_proxies = []
    servers = set()
    node_list = {}
    node_name = set()
    for url in url_lists:
        try:
            # ä½¿ç”¨ lock ç¡®ä¿å¤šè¿›ç¨‹è°ƒç”¨æ—¶æ•°æ®å®‰å…¨
            with threading.Lock():
                nodes = decode_url.decode_url_to_nodes(url)
                if nodes:
                    new_proxies.extend(nodes)
                    logging.info(f"Successfully parsed {len(nodes)} nodes from {url}")
                    if len(nodes) > 0:
                        continue
        except Exception as e:
            logging.error(f"Error processing URL {url}: {str(e)}")
            pass
        url_quote = urllib.parse.quote(url, safe='')
        # config_quote = urllib.parse.quote(config_url, safe='')
        # include_quote = urllib.parse.quote(include, safe='')
        exclude_quote = urllib.parse.quote(exclude, safe='')
        # è½¬æ¢å¹¶è·å–è®¢é˜…é“¾æ¥æ•°æ®
        converted_url = server_host + '/sub?target=clash&url=' + url_quote + \
                        '&emoji=true&list=true&tfo=true&scv=true&fdn=true&sort=false&new_name=true&exclude=' + exclude_quote
        try:
            # lock.acquire()
            s = requests.Session()
            s.mount('http://', HTTPAdapter(max_retries=5))
            s.mount('https://', HTTPAdapter(max_retries=5))
            resp = s.get(converted_url, timeout=30)
            # å¦‚æœè§£æå‡ºé”™ï¼Œå°†åŸå§‹é“¾æ¥å†…å®¹æ‹·è´ä¸‹æ¥
            text = resp.text
            try:
                text.encode('utf-8')
                yaml_text = yaml.safe_load(text)
            except Exception as err:
                logging.error(f"{url} {err.args[0]}")
                continue
            if 'No nodes were found!' in text:
                logging.error("%s No nodes were found!", url)
                continue
            if 'The following link' in text:
                logging.error("%s The following link!", url)
                continue
            if '414 Request-URI Too Large' in text:
                logging.error("%s 414 Request-URI Too Large!", url)
                continue
            if yaml_text is None:
                logging.error("%s is None!", url)
                continue
            if yaml_text is not None and 'proxies' in yaml_text.keys():
                proxies = yaml_text['proxies']
                logging.info(f"{url}    {len(proxies)}")
                random.shuffle(proxies)
                for proxie in proxies:
                    try:
                        server = proxie['server']
                        # port = proxie['port']
                        # sp = str(server) + ":" + str(port)
                        # if not test_connection(server, port):
                        #     servers.add(sp)
                        #     not_proxies.add(proxie['server'])
                        #     continue
                        # if sp in servers:
                        #     not_proxies.add(proxie['server'])
                        #     continue
                        # else:
                        #     servers.add(sp)
                        name = proxie['name']
                        if name not in node_name:
                            node_name.add(name)
                        else:
                            name = name + str(len(node_name))
                            proxie['name'] = name
                        # Special handling for SS nodes without obfs parameter
                        if proxie.get('type') == 'ss' and 'obfs' not in proxie:
                            # SS nodes without obfs parameter should not be removed
                            not_proxies.add(proxie['server'])
                            continue
                        # TLS must be true with h2/ grpc network
                        if "network" in proxie.keys() and "tls" in proxie.keys():
                            network = proxie['network']
                            tls = proxie['tls']
                            if network == "h2" or network == "grpc":
                                if tls is False:
                                    not_proxies.add(proxie['server'])
                                    continue
                        if "cipher" in proxie.keys() and proxie['cipher'] not in cipher_list:
                            not_proxies.add(proxie['server'])
                            continue
                        if server in exce_url:
                            not_proxies.add(proxie['server'])
                            continue
                        if server.startswith("127") or server.startswith("192") or server.startswith("10."):
                            not_proxies.add(proxie['server'])
                            continue
                        if "uuid" in proxie.keys() and len(proxie['uuid']) != 36:
                            not_proxies.add(proxie['server'])
                            continue
                        # æ ¡éªŒprotocol-paramæ˜¯å¦æ­£å¸¸
                        if "protocol-param" in proxie.keys():
                            try:
                                proxie['protocol-param'] = base64.b64decode(proxie['protocol-param']).decode('utf-8')
                            except Exception as e:
                                not_proxies.add(proxie['server'])
                                continue
                        
                        # add name emoji
                        if not has_emoji(name):
                            c_emoji = get_country_emoji(server)
                            if c_emoji is not None:
                                proxie['name'] = f"{c_emoji} {name}"
                            else:
                                not_proxies.add(proxie['server'])
                                continue
                        new_proxies.append(proxie)
                    except Exception as e:
                        not_proxies.add(proxie['server'])
                        logging.error(f"proxie:{proxie} error:{e.args[0]}")
                        continue
        except Exception as err:
            # é“¾æ¥æœ‰é—®é¢˜ï¼Œç›´æ¥è¿”å›åŸå§‹é”™è¯¯
            logging.error(f"url:{url}  error:{err.args[0]}")
            continue
        # finally:
        # lock.release()
        continue
    try:
        # lock.acquire()
        if new_proxies is not None:
            shared_list.extend(new_proxies)
            logging.info("%d Number of nodes after filtering:%d", index, len(new_proxies))
            logging.info("%d Number of discarded nodes:%d", index, len(not_proxies))
            # with open(yaml_file, "w", encoding="utf-8") as f:
            #     node_list['proxies'] = new_proxies
            #     # f.write(yaml.dump(node_list))
            #     yaml.safe_dump(node_list, f, allow_unicode=True)
        else:
            logging.error("%d is empty", index)
    except Exception as e:
        # é“¾æ¥æœ‰é—®é¢˜ï¼Œç›´æ¥è¿”å›åŸå§‹é”™è¯¯
        logging.error("%d ERROR %s", index, e.args[0])
    # finally:
    #     lock.release()


def split_node(n, shared_list):
    node_list = {}
    yaml_file = "./sub/" + str(n) + ".yaml"
    name_list = []
    if shared_list is None:
        logging.error("shared_list is None")
        return
    if n > 100:
        return
    for list in shared_list:
        name = list['name']
        if name not in name_list:
            name_list.append(name)
        else:
            name = name + str(len(name_list))
            list['name'] = name
    with open(yaml_file, "w", encoding="utf-8") as f:
        node_list['proxies'] = shared_list
        yaml.safe_dump(node_list, f, allow_unicode=True)


if __name__ == '__main__':
    # åˆ›å»ºå¤šä¸ªè¿›ç¨‹
    processes = []
    manager = multiprocessing.Manager()
    shared_list = manager.list()
    for i in range(thread_num):
        p = multiprocessing.Process(target=run, args=(i, shared_list,))
        processes.append(p)
        p.start()
    logging.info("å¤šè¿›ç¨‹å·²å¯åŠ¨")

    threshold = 5000  # èŠ‚ç‚¹é˜ˆå€¼ï¼Œè¾¾åˆ°åˆ™åœæ­¢æ‰€æœ‰å­è¿›ç¨‹
    try:
        # ç›‘æ§å­è¿›ç¨‹ä¸ shared_list é•¿åº¦
        while any(p.is_alive() for p in processes):
            if len(shared_list) >= threshold:
                logging.warning("èŠ‚ç‚¹æ•°è¾¾åˆ°é˜ˆå€¼ %dï¼Œåœæ­¢æ‰€æœ‰å­è¿›ç¨‹", threshold)
                for p in processes:
                    if p.is_alive():
                        p.terminate()
                break
            time.sleep(1)
    except KeyboardInterrupt:
        logging.warning("æ”¶åˆ°ä¸­æ–­ï¼Œç»ˆæ­¢å­è¿›ç¨‹")
        for p in processes:
            if p.is_alive():
                p.terminate()
    finally:
        for p in processes:
            p.join()

    logging.info("å¤šè¿›ç¨‹å·²ç»“æŸï¼Œå½“å‰èŠ‚ç‚¹æ•°ï¼š%d", len(shared_list))
    random.shuffle(shared_list)
    each_num = 500
    thread_list = []
    t_num = len(shared_list) // each_num + 1
    for i in range(t_num):
        if (i + 1) * each_num <= len(shared_list):
            t = threading.Thread(target=split_node, args=(i, shared_list[i * each_num:i * each_num + each_num]))
        else:
            t = threading.Thread(target=split_node, args=(i, shared_list[i * each_num:]))
        thread_list.append(t)
        t.start()
    logging.info("%d threads actived", threading.active_count() - 1)
    for thread in thread_list:
        thread.join()
    
    # è¾“å‡ºæœ€ç»ˆçš„èŠ‚ç‚¹ç»Ÿè®¡
    logging.info("=== Final Summary ===")
    logging.info(f"Total nodes collected: {len(shared_list)}")
    logging.info(f"Average nodes per file: {len(shared_list) // t_num}")
    logging.info(f"Number of split files: {t_num}")
    logging.info("All processes have finished.")
